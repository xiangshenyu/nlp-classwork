{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPrNFh+qyLZ9/aJN/3JO9ZN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xiangshenyu/nlp-classwork/blob/main/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6zwyUxjzdMh"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 安装 LangChain 全家桶、向量库 ChromaDB、HuggingFace 组件\n",
        "!pip install -q langchain langchain-community langchain-huggingface chromadb\n",
        "# 安装模型加载和量化所需的库 (在 GPU 上跑模型必须)\n",
        "!pip install -q transformers accelerate bitsandbytes sentence-transformers"
      ],
      "metadata": {
        "id": "u0sSKPsa0B_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
        "\n",
        "# ================= 1. 准备数据 (模拟知识库) =================\n",
        "# 这里直接创建一个文本文件作为演示。如果是作业，你可以替换为你自己的 txt 或 pdf 读取逻辑。\n",
        "with open(\"knowledge.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\"\"\n",
        "    关于“极光项目”的内部资料：\n",
        "    极光项目（Project Aurora）是公司在2025年启动的秘密研发计划。\n",
        "    该项目旨在开发下一代量子加密通讯设备。\n",
        "    项目负责人是李博士，核心实验室位于瑞士苏黎世。\n",
        "    目前的进度是：原型机已完成测试，预计2026年Q3推向市场。\n",
        "    注意：该项目的代号在内部文件中通常简写为 PA-25。\n",
        "    \"\"\")\n",
        "\n",
        "print(\"正在加载和处理文档...\")\n",
        "loader = TextLoader(\"knowledge.txt\", encoding=\"utf-8\")\n",
        "docs = loader.load()\n",
        "\n",
        "# 文本切分：将长文档切成小块\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "# ================= 2. 加载 Embedding 模型 & 向量数据库 =================\n",
        "print(\"正在加载 Embedding 模型 (BGE-Small)...\")\n",
        "# 使用 BGE 中文模型\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-zh-v1.5\")\n",
        "\n",
        "print(\"正在构建向量数据库...\")\n",
        "# 将切分后的文本向量化并存入 ChromaDB (内存模式)\n",
        "vectorstore = Chroma.from_documents(documents=splits, embedding=embedding_model)\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2}) # 检索最相关的2个片段\n",
        "\n",
        "# ================= 3. 加载 LLM (使用 4-bit 量化以适应 Colab 显存) =================\n",
        "print(\"正在加载 LLM (Qwen2.5-7B)...这可能需要几分钟下载模型...\")\n",
        "\n",
        "model_id = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "\n",
        "# 4-bit 量化配置\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\", # 自动分配到 GPU\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# 创建 HuggingFace Pipeline\n",
        "text_generation_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=256,  # 生成回答的最大长度\n",
        "    temperature=0.1,     # 温度越低，回答越严谨\n",
        "    repetition_penalty=1.1\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
        "\n",
        "# ================= 4. 构建 RAG 链 =================\n",
        "# 定义 Prompt 模板\n",
        "template = \"\"\"你是一个智能助手。请严格根据下面的【上下文】回答【问题】。\n",
        "如果上下文中没有答案，请直接说不知道，不要杜撰。\n",
        "\n",
        "【上下文】：\n",
        "{context}\n",
        "\n",
        "【问题】：\n",
        "{question}\n",
        "\n",
        "【答案】：\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "# LangChain 经典的 LCEL 链式写法\n",
        "rag_chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# ================= 5. 测试运行 =================\n",
        "print(\"\\n=== 系统初始化完成，开始测试 ===\\n\")\n",
        "\n",
        "questions = [\"极光项目的负责人是谁？\", \"PA-25是什么意思？\", \"极光项目在研究什么？\", \"项目在法国吗？\"]\n",
        "\n",
        "for q in questions:\n",
        "    print(f\"问: {q}\")\n",
        "    # invoke 触发检索和生成\n",
        "    result = rag_chain.invoke(q)\n",
        "\n",
        "    # 清理一下输出，有时候模型会把 Prompt 也打印出来，我们只需要生成的答案部分\n",
        "    # 这里做一个简单的字符串处理，只取“【答案】：”之后的内容（如果模型遵循指令较好的话）\n",
        "    if \"【答案】：\" in result:\n",
        "        clean_result = result.split(\"【答案】：\")[-1].strip()\n",
        "    else:\n",
        "        # 如果模型没有严格按格式输出，就尝试截取 Answer 之后的部分，或者直接打印\n",
        "        # Qwen 通常指令跟随能力很强，但也可能直接接着生成\n",
        "        clean_result = result\n",
        "        # 简单的清理逻辑：去掉 Prompt 部分\n",
        "        if template.split(\"【答案】：\")[0] in clean_result:\n",
        "             clean_result = clean_result.replace(template.split(\"【答案】：\")[0], \"\")\n",
        "\n",
        "    print(f\"答: {clean_result}\")\n",
        "    print(\"-\" * 30)"
      ],
      "metadata": {
        "id": "9Ze1YVLw0CRn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}